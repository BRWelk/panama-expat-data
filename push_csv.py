#!/usr/bin/env python3
# push_csv.py ‚Äî build CSV ‚Üí git commit/push ‚Üí Airtable upsert (one script)

import os, re, sys, csv, json, subprocess, requests, argparse
from pathlib import Path
from datetime import datetime
from urllib.parse import quote

# ---------------------------
# Paths & constants
# ---------------------------
REPO_DIR = Path(__file__).resolve().parent
ENV_PATH = REPO_DIR / ".env.local"
CSV_OUT  = REPO_DIR / "data" / "panama_rent_averages.csv"
SNAPSHOT_DIR = REPO_DIR / "data"
SNAPSHOT_NAME_FMT = "panama_rent_averages_{yyyy}-{mm}.csv"  # e.g., ..._2025-10.csv

# ---------------------------
# Env handling
# ---------------------------
def parse_env_file(path: Path):
    if not path.exists():
        return {}
    env = {}
    for raw in path.read_text(encoding="utf-8").splitlines():
        line = raw.strip()
        if not line or line.startswith("#") or "=" not in line: 
            continue
        k, v = line.split("=", 1)
        env[k.strip()] = re.sub(r"\s+#.*$", "", v).strip().strip('"').strip("'")
    return env

def get_env():
    file_env = parse_env_file(ENV_PATH)
    # process env takes precedence if set
    env = dict(file_env)
    for k in list(file_env.keys()):
        if k in os.environ and os.environ[k]:
            env[k] = os.environ[k]
    return env

# ---------------------------
# CSV BUILD (replace with your real generator if needed)
# ---------------------------
def build_csv_if_needed(skip_build: bool) -> None:
    """
    Default behavior: if --skip-build is passed we do nothing.
    If not skipping and CSV already exists, we keep it (you may replace with your true generator).
    If not skipping and CSV is missing, we create an empty headered CSV (so pipeline still runs).
    Replace this function with your real builder if you want automatic regeneration.
    """
    if skip_build:
        print("‚è≠Ô∏è  Skipping CSV build (per flag).")
        return
    if CSV_OUT.exists():
        print(f"‚úÖ CSV exists: {CSV_OUT}")
        return
    print(f"‚ö†Ô∏è  CSV not found; creating a header-only file at: {CSV_OUT}")
    CSV_OUT.parent.mkdir(parents=True, exist_ok=True)
    headers = [
        "Date","City/Neighborhood","Configuration","Average Price (USD)",
        "Utilities","Groceries","Internet","Cell Phone","Dining","Entertainment","Travel"
    ]
    with open(CSV_OUT, "w", newline="", encoding="utf-8") as f:
        writer = csv.writer(f)
        writer.writerow(headers)

# ---------------------------
# Snapshot
# ---------------------------
def write_monthly_snapshot(force_snapshot: bool) -> None:
    yyyy = datetime.now().strftime("%Y")
    mm   = datetime.now().strftime("%m")
    snap_name = SNAPSHOT_NAME_FMT.format(yyyy=yyyy, mm=mm)
    snap_path = SNAPSHOT_DIR / snap_name
    if snap_path.exists() and not force_snapshot:
        print(f"‚ÑπÔ∏è Snapshot exists, skipping: {snap_name} (use --force-snapshot to overwrite)")
        return
    data = CSV_OUT.read_bytes()
    snap_path.write_bytes(data)
    print(f"‚úÖ Wrote snapshot: {snap_path}")

# ---------------------------
# Git helpers
# ---------------------------
def run(cmd, cwd=None, check=True):
    res = subprocess.run(cmd, cwd=cwd, capture_output=True, text=True, shell=False)
    if check and res.returncode != 0:
        print("‚ùå Command failed:", " ".join(cmd))
        print(res.stdout)
        print(res.stderr)
        sys.exit(res.returncode)
    return res

def ensure_git_setup():
    run(["git", "--version"])
    # ensure we're in a repo
    run(["git", "rev-parse", "--is-inside-work-tree"], cwd=str(REPO_DIR))

def git_commit_and_push(commit_msg: str):
    run(["git", "add", str(CSV_OUT)], cwd=str(REPO_DIR))
    # include snapshot if present
    for p in SNAPSHOT_DIR.glob("panama_rent_averages_*.csv"):
        run(["git", "add", str(p)], cwd=str(REPO_DIR))
    # commit (no error if nothing to commit)
    res = subprocess.run(["git", "commit", "-m", commit_msg], cwd=str(REPO_DIR), capture_output=True, text=True)
    if "nothing to commit" in (res.stdout + res.stderr).lower():
        print("‚ÑπÔ∏è Nothing to commit.")
    else:
        print(res.stdout.strip() or res.stderr.strip())
    run(["git", "push", "origin", "HEAD"], cwd=str(REPO_DIR))
    print("üöÄ Pushed to origin.")

# ---------------------------
# Airtable upsert
# ---------------------------
def airtable_upsert_from_csv(env: dict) -> None:
    import requests, csv, json
    token = env.get("AIRTABLE_TOKEN")
    base  = env.get("AIRTABLE_BASE")
    table = env.get("AIRTABLE_RENTS_TABLE") or env.get("AIRTABLE_TABLE") or "Rent"

    if not (token and base and table):
        print("‚ö†Ô∏è  Skipping Airtable upsert: missing AIRTABLE_TOKEN / AIRTABLE_BASE / table id/name")
        return
    if not CSV_OUT.exists():
        print(f"‚ö†Ô∏è  Skipping Airtable upsert: CSV not found at {CSV_OUT}")
        return

    # --- Define both schemas we support ---
    LONG_FIELDS = [
        "Date","City/Neighborhood","Configuration","Average Price (USD)",
        "Utilities","Groceries","Internet","Cell Phone","Dining","Entertainment","Travel"
    ]
    LONG_KEYS = ["Date","City/Neighborhood","Configuration"]
    LONG_NUM = {"Average Price (USD)","Utilities","Groceries","Internet","Cell Phone","Dining","Entertainment","Travel"}

    SHORT_FIELDS = ["City","Bedrooms","Range","USD_Rent"]
    SHORT_KEYS = ["City","Bedrooms","Range"]
    SHORT_NUM = {"USD_Rent"}

    # --- Load CSV + sniff headers ---
    with open(CSV_OUT, newline="", encoding="utf-8") as f:
        rows = list(csv.DictReader(f))
        csv_headers = [h.strip() for h in (rows[0].keys() if rows else [])]

    # --- Probe table to see what fields exist there (no schema API, so inspect records) ---
    base_url = f"https://api.airtable.com/v0/{base}/{quote(str(table), safe='')}"
    hdrs = {"Authorization": f"Bearer {token}", "Accept": "application/json"}
    probe = requests.get(base_url, headers=hdrs, params={"maxRecords": 3}, timeout=30)
    if probe.status_code >= 300:
        print("‚ùå Could not probe Airtable table:", probe.status_code, probe.text)
        raise SystemExit(1)

    recs = probe.json().get("records", [])
    table_fields = set()
    for r in recs:
        table_fields.update((r.get("fields") or {}).keys())

    # Helper: choose a schema that BOTH the CSV and the table can satisfy
    def choose_schema():
        csv_has_long = set(LONG_FIELDS).issubset(set(csv_headers))
        csv_has_short = set(SHORT_FIELDS).issubset(set(csv_headers))
        tbl_has_long = set(LONG_FIELDS) & table_fields  # some tables may be empty
        tbl_has_short = set(SHORT_FIELDS) & table_fields

        # Prefer long if CSV supports it AND merge keys exist (or table empty)
        if csv_has_long and (not table_fields or set(LONG_KEYS).issubset(table_fields)):
            return "long"
        # Otherwise fallback to short if possible
        if csv_has_short and (not table_fields or set(SHORT_KEYS).issubset(table_fields)):
            return "short"
        # Last resort: if table has no records (empty), use long if CSV supports it; else short
        if not table_fields:
            if csv_has_long: return "long"
            if csv_has_short: return "short"
        return None

    schema = choose_schema()
    if not schema:
        print("‚ùå CSV ‚Üî Airtable mismatch.")
        print("   CSV headers:", csv_headers)
        print("   Sample Airtable fields:", sorted(table_fields) if table_fields else "(no rows yet)")
        print("   Expected LONG:", LONG_FIELDS)
        print("   Expected SHORT:", SHORT_FIELDS)
        raise SystemExit(1)

    if schema == "long":
        FIELD_MAP = {k: k for k in LONG_FIELDS}
        PRIMARY_KEY_FIELDS = LONG_KEYS
        NUMERIC_FIELDS = LONG_NUM
    else:
        FIELD_MAP = {k: k for k in SHORT_FIELDS}
        PRIMARY_KEY_FIELDS = SHORT_KEYS
        NUMERIC_FIELDS = SHORT_NUM

    # Final check that merge keys exist on table if the table isn‚Äôt empty
    if table_fields and not set(PRIMARY_KEY_FIELDS).issubset(table_fields):
        # Downgrade to short if possible
        if schema == "long" and set(SHORT_KEYS).issubset(set(csv_headers)) and set(SHORT_KEYS).issubset(table_fields):
            print("‚ÑπÔ∏è Merge keys for LONG schema not found in table; falling back to SHORT schema.")
            FIELD_MAP = {k: k for k in SHORT_FIELDS}
            PRIMARY_KEY_FIELDS = SHORT_KEYS
            NUMERIC_FIELDS = SHORT_NUM
            schema = "short"
        else:
            print("‚ùå Merge keys not present in Airtable table:", PRIMARY_KEY_FIELDS)
            print("   Table fields:", sorted(table_fields))
            raise SystemExit(1)

    def coerce(name, val):
        if val is None or val == "": return None
        if name == "Date":
            for fmt in ("%m/%d/%Y","%Y-%m-%d","%m/%d/%y"):
                try: return datetime.strptime(val, fmt).strftime("%Y-%m-%d")
                except: pass
            return val
        if name in NUMERIC_FIELDS:
            try: return float(str(val).replace(",", ""))
            except: return val
        return val

    # Build records aligned to chosen FIELD_MAP
    payload_records = []
    for r in rows:
        fields = {}
        for csv_name, at_name in FIELD_MAP.items():
            if csv_name in r:
                fields[at_name] = coerce(at_name, r[csv_name])
        if fields:
            payload_records.append({"fields": fields})

    if not payload_records:
        print("‚ö†Ô∏è  No rows to upsert (CSV empty or headers mismatch).")
        return

    # Upsert in chunks
    upsert_url = f"{base_url}?typecast=true"
    headers_json = {"Authorization": f"Bearer {token}", "Content-Type": "application/json"}
    payload_template = {"performUpsert": {"fieldsToMergeOn": [{"fieldName": n} for n in PRIMARY_KEY_FIELDS]}}

    sent = 0
    for i in range(0, len(payload_records), 10):
        batch = payload_records[i:i+10]
        payload = dict(payload_template)
        payload["records"] = batch
        res = requests.patch(upsert_url, headers=headers_json, data=json.dumps(payload), timeout=30)
        if res.status_code >= 300:
            print("‚ùå Airtable error:", res.status_code, res.text)
            # Extra debug: show the first record of the batch that failed
            print("   Example record:", json.dumps(batch[0], ensure_ascii=False))
            raise SystemExit(1)
        sent += len(batch)

    print(f"‚úÖ [{schema}] Upserted {sent} records into Airtable table '{table}'.")


if __name__ == "__main__":
    main()






